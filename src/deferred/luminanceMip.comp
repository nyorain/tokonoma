#version 450

// each workgroup reduces groupDimSize * 2 pixels in each dimension.
// therefore expects the output texture to be smaller by this factor
// e.g. if the specilization constant groupDimSize is 8 
// (solid choice usually), dimensions of outLim are smaller by factor 16
// (rounded up though)
// Can e.g. use 4th next mip level in that case (or the 3th next in case
// the 4th is to small - can happen with rounding up since mipmaps sizes
// are rounded down)

// TODO: can probably be optimzied. Especially if just choose a fixed workgroup
// size, then we can unroll the loop which should help with performance.
// Otherwise, a good driver should be able to do it as well...
// http://developer.download.nvidia.com/compute/cuda/1_1/Website/projects/reduction/doc/reduction.pdf

// NOTE: we assume that x and y work group sizes are the same.
// There is no reason why we should choose something else and it
// simplifies the computation here tremendously
layout(local_size_x_id = 0, local_size_y_id = 0) in;
layout(set = 0, binding = 0) uniform sampler2D inLum;
layout(set = 0, binding = 1, r16f) uniform writeonly image2D outLum;

layout(push_constant) uniform PCR {
	uvec2 inSize; // input size
	// how much the last invocation in x or y is worth
	vec2 last;
} pcr;

// constant for all invocations
const uint size = gl_WorkGroupSize.x; // == gl_WorkGroupSize.y
vec2 pixelSize = 1.f / textureSize(inLum, 0);

// TODO
vec2 lastGroupSize = pcr.inSize - 2 * size * gl_WorkGroupID.xy - 1 + pcr.last;
vec2 pixelCount = min(lastGroupSize, 2 * size.xx);

// contain the current summed-up luminance
shared float lum[size][size];

float load(vec2 pixel) {
	vec2 off = 0.5 * vec2(lessThan(pixel + 1, pcr.inSize));
	// TODO: can probably be done via pure math instead of switches
	off.x += 0.5 * ((pixel.x + 2 >= pcr.inSize.x) ? pcr.last.x : 1.f);
	off.y += 0.5 * ((pixel.y + 2 >= pcr.inSize.y) ? pcr.last.y : 1.f);
	float fac = 4 * off.x * off.y;
	pixel += off;
	return fac * texture(inLum, pixel * pixelSize).r; // initial load
}

// NOTE: no early returns due to all the barriers. We use a sampler
// with a black border and clampToBorder instead.
void main() {
	uvec2 l = gl_LocalInvocationID.xy;
	vec2 pixel = 2 * gl_GlobalInvocationID.xy; // top-left of sampled pixels

	lum[l.x][l.y] = load(pixel); // initial load
	for(uint isize = size / 2; isize > 0; isize /= 2) {
		// wait for initial load/last vertical reduce to complete
		// apparently both barriers needed:
		// https://stackoverflow.com/questions/39393560
		memoryBarrierShared();
		barrier();

		if(l.x < isize) {
			lum[l.x][l.y] += lum[isize + l.x][l.y];
		}

		// wait for horizontal reduce above to complete
		memoryBarrierShared();
		barrier();
		if(l.y < isize) {
			lum[l.x][l.y] += lum[l.x][isize + l.y];
		}
	}

	// there is only one invocation active at this point: l = (0, 0)
	float avg = lum[0][0] / (pixelCount.x * pixelCount.y);
	imageStore(outLum, ivec2(gl_WorkGroupID.xy), vec4(avg));
}
